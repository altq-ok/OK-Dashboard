"""
FastAPI Application Entry Point.

Handles the lifecycle of the background worker process and provides
REST endpoints for the Next.js frontend to interact with the calculation engine.
"""

import json
import logging
import os
import pathlib
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from multiprocessing import Event, Process, Queue
from os.path import expanduser

import portalocker
from fastapi import FastAPI, HTTPException, Request
from starlette.middleware.cors import CORSMiddleware

from app.core.data_manager import DataManager
from app.core.status import StatusManager
from app.core.syncer import FileSyncer
from app.core.worker import calc_worker
from app.schemas.task import TaskParams, TaskStatus, UserEvent

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Configuration
home = pathlib.Path(expanduser("~"))
dev_dir = home / "Dev"
if not dev_dir.exists():
    dev_dir = home / "Documents" / "Dev"

SHARED_DIR = str(dev_dir / "TypeScript" / "Shared")
LOCAL_DIR = str(dev_dir / "TypeScript" / "Local")
USER_NAME = os.getlogin()
# Directly write to the shared drive as this should be small and not frequent
USER_DATA_DIR = pathlib.Path(SHARED_DIR) / "user_data"
USER_EVENTS_FILE = USER_DATA_DIR / "user_events.json"

# To save under app.state
state = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifecycle handler to manage startup and shutdown of FastAPI
    Before yield => on startup
    After yield => on shutdown
    """
    # Monitor Worker's readiness
    ready_event = Event()

    # Initialize managers
    # StatusManager and DataManager will monitor local mirror of shared
    app.state.status_manager = StatusManager(LOCAL_DIR)
    app.state.data_manager = DataManager(LOCAL_DIR)

    # Start file syncer thread
    syncer = FileSyncer(SHARED_DIR, LOCAL_DIR, interval=5)
    syncer.start()
    app.state.syncer = syncer

    # Start a worker process
    task_queue = Queue()  # Queue for IPC between FastAPI and Worker
    worker_process = Process(target=calc_worker, args=(task_queue, SHARED_DIR, USER_NAME, ready_event), daemon=True)
    worker_process.start()

    # Save them to app.state, so each API endpoint can access
    app.state.task_queue = task_queue
    app.state.worker_process = worker_process
    app.state.worker_ready_event = ready_event

    logger.info(f"Worker process started with PID {worker_process.pid}")

    yield  # FastAPI starts up here and wait for a request

    # Cleanup on FastAPI's shutdown
    logger.info("Worker process shutting down. Cleaning up resources...")
    if app.state.worker_process.is_alive():
        app.state.worker_process.terminate()
        app.state.worker_process.join()

    # Stop syncer's thread
    if hasattr(app.state.syncer, "stop"):
        app.state.syncer.stop()

    logger.info("Worker process cleanup complete")


# Set lifespan on app creation
app = FastAPI(lifespan=lifespan)

# CORS to allow an access from frontend
app.add_middleware(CORSMiddleware, allow_origins=["http://localhost:3000"], allow_methods=["*"], allow_headers=["*"])


@app.post("/run-task")
async def run_task(params: TaskParams, request: Request):
    """
    Submits a task to the local worker queue.
    The task_id is generated by combining target_id and task_type.
    """
    task_id = f"{params.target_id}_{params.task_type}_{USER_NAME}"
    try:
        request.app.state.task_queue.put(
            {
                "task_id": task_id,
                "params": params.model_dump(),  # Convert Pydantic to dict for Queue
            }
        )
        return {"status": "accepted", "task_id": task_id}
    except Exception as e:
        logger.error(f"Failed to submit task: {e}")
        raise HTTPException(status_code=500, detail="Internal worker queue error")


@app.get("/tasks/status", response_model=list[TaskStatus])
async def get_all_task_statuses(request: Request):
    """
    Returns the status of all tracked tasks in the team's shared directory.
    Used by the Global Task Monitor (Dropdown).
    """
    return request.app.state.status_manager.get_all_statuses()


@app.get("/tasks/{target_id}/{task_type}/status", response_model=TaskStatus)
async def get_task_status(target_id: str, task_type: str, request: Request):
    """
    Returns status for a specific task. Used for widget-level polling.
    """
    all_statuses = request.app.state.status_manager.get_all_statuses()
    relevant = [
        s for s in all_statuses if s["params"]["target_id"] == target_id and s["params"]["task_type"] == task_type
    ]
    if not relevant:
        raise HTTPException(status_code=404, detail="No status found for this task")

    # Sort by 'last_heartbeat' and status ('running' at top)
    relevant.sort(key=lambda x: (x["status"] == "running", x["last_heartbeat"]), reverse=True)
    return relevant[0]


@app.get("/data/{target_id}/{data_type}/snapshots")
async def get_data_snapshots(target_id: str, data_type: str, request: Request):
    """
    Returns a list of available parquet snapshot filenames for a data type.
    """
    snapshots = request.app.state.data_manager.get_snapshots(data_type, target_id)
    return {"snapshots": snapshots}


@app.get("/data/{target_id}/{data_type}/content")
async def get_data_content(target_id: str, data_type: str, request: Request, version: str = "latest"):
    """
    Returns the actual content of a parquet snapshot as a JSON-serializable list.
    Supports historical data retrieval via the 'version' query parameter.
    """
    data_manager = request.app.state.data_manager
    try:
        if version == "latest":
            data = data_manager.get_latest_data(data_type, target_id)
        else:
            df = data_manager.load_parquet(data_type, target_id, version)
            data = df.to_dict(orient="records")
        return {"data": data}
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Snapshot file not found")
    except Exception as e:
        logger.error(f"Error reading parquet: {e}")
        raise HTTPException(status_code=500, detail="Error processing data file")


@app.post("/stop-worker")
async def stop_worker(request: Request):
    """
    Manually restarts the worker process to clear memory or recover from hangs.
    """
    if request.app.state.worker_process.is_alive():
        request.app.state.worker_process.terminate()
        request.app.state.worker_process.join()

    # Re-spawn the process with the existing queue
    new_ready_event = Event()
    new_process = Process(
        target=calc_worker, args=(request.app.state.task_queue, SHARED_DIR, USER_NAME, new_ready_event), daemon=True
    )
    new_process.start()
    request.app.state.worker_process = new_process
    request.app.state.worker_ready_event = new_ready_event

    return {"status": "restarted", "new_pid": new_process.pid}


@app.get("/api/health")
async def health_check():
    """Simple API health check with basic metadata."""
    return {
        "status": "ok",
        "user": USER_NAME,
        "timestamp_utc": datetime.now(timezone.utc).isoformat(),
    }


@app.get("/system/worker-status")
async def get_worker_status(request: Request):
    process = request.app.state.worker_process
    ready_event = request.app.state.worker_ready_event

    is_alive = process.is_alive() if process else False
    # Check if process is alive and also ready_event is set
    is_ready = is_alive and ready_event.is_set()
    return {
        "is_alive": is_alive,
        "pid": process.pid if is_alive else None,
        "status": "ready" if is_ready else "initializing" if is_alive else "offline",
    }


@app.get("/data/user-events", response_model=list[UserEvent])
async def get_user_events():
    """Get user input events from the shared"""
    if not USER_EVENTS_FILE.exists():
        return []
    try:
        with open(USER_EVENTS_FILE, "r", encoding="utf-8") as f:
            # Shared lock - allow multiple reads but one write
            portalocker.lock(f, portalocker.LOCK_SH)
            content = f.read()
            return json.loads(content) if content else []
    except Exception as e:
        logger.error(f"Error reading user events: {e}")
        return []


@app.post("/data/user-events")
async def save_user_event(event: UserEvent):
    """Write new event to JSON in the shared"""
    USER_DATA_DIR.mkdir(parents=True, exist_ok=True)
    try:
        with open(USER_EVENTS_FILE, "a+", encoding="utf-8") as f:
            portalocker.lock(f, portalocker.LOCK_EX)  # Exclusive lock
            # Read from the top of the file
            f.seek(0)
            content = f.read()
            events = json.loads(content) if content else []

            # Update if ID exists, or add a new item
            event_dict = event.model_dump()
            existing_idx = next((i for i, e in enumerate(events) if e["event_id"] == event.event_id), None)
            if existing_idx is not None:
                events[existing_idx] = event_dict
            else:
                events.append(event_dict)

            # Write to the file
            f.seek(0)
            f.truncate()
            json.dump(events, f, indent=2, ensure_ascii=False)

        return {"status": "success"}
    except Exception as e:
        logger.error(f"Error saving user event: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/data/user-events/{event_id}")
async def delete_user_event(event_id: str):
    """Delete an event with ID"""
    if not USER_EVENTS_FILE.exists():
        return {"status": "not_found"}

    with open(USER_EVENTS_FILE, "a+", encoding="utf-8") as f:
        portalocker.lock(f, portalocker.LOCK_EX)
        f.seek(0)
        content = f.read()

        # Remove the item with the ID
        events = json.loads(content) if content else []
        new_events = [e for e in events if e["event_id"] != event_id]

        # Write to the file
        f.seek(0)
        f.truncate()
        json.dump(new_events, f, indent=2, ensure_ascii=False)

    return {"status": "deleted"}
